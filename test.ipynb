{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from dataloader import RGBDepthPano\n",
    "\n",
    "from image_encoders import RGBEncoder, DepthEncoder\n",
    "from TRM_net import BinaryDistPredictor_TRM, TRM_predict\n",
    "\n",
    "from eval import waypoint_eval\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import utils\n",
    "import random\n",
    "from utils import nms\n",
    "from utils import print_progress\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.EXP_ID = 'test_ipynb'\n",
    "        self.TRAINEVAL = 'train'\n",
    "        self.VIS = 0\n",
    "        self.ANGLES = 120\n",
    "        self.NUM_IMGS = 12\n",
    "        self.NUM_CLASSES = 12\n",
    "        self.MAX_NUM_CANDIDATES = 5\n",
    "        self.PREDICTOR_NET = 'TRM'\n",
    "        self.EPOCH = 300\n",
    "        self.BATCH_SIZE = 8\n",
    "        self.LEARNING_RATE = 1e-6\n",
    "        self.WEIGHT = 0\n",
    "        self.TRM_LAYER = 2\n",
    "        self.TRM_NEIGHBOR = 1\n",
    "        self.HEATMAP_OFFSET = 5\n",
    "        self.HIDDEN_DIM = 768\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Set random seeds and create experiment directories\n",
    "    Args:\n",
    "        args: Command line arguments\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)  # Set PyTorch random seed\n",
    "    random.seed(0)  # Set Python random seed\n",
    "    exp_log_path = './checkpoints/%s/'%(args.EXP_ID)  # Experiment log path\n",
    "    os.makedirs(exp_log_path, exist_ok=True)  # Create experiment directory\n",
    "    exp_log_path = './checkpoints/%s/snap/'%(args.EXP_ID)  # Model snapshot path\n",
    "    os.makedirs(exp_log_path, exist_ok=True)  # Create model snapshot directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBEncoder \n",
    "It uses Torchvision pre-trained Resnet50. It removes last two layers (fully connected and pooling layers), keep only feature extraction part. \n",
    "* Input size: [batchsize, num_imgs, 3, 224, 224]\n",
    "* Output size: [batchsize*num_imgs, 2048, 7, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_encoder = RGBEncoder(resnet_pretrain=True, trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DepthEncoder\n",
    "It is based on habitat_baselines.rl.ddppo.policy resnet, specificly resnet50. It first goes through the backbone net where you can specific the output channels after first conv, parameters of Group Norm and input image size. Then it goes through a compression layer to change the number of channels so that the output flatten size is what you want, which is after_compression_flat_size.\n",
    "* Input size: [batchsize, num_imgs, 256, 256, 1]\n",
    "* Output size: [batchsize*num_imgs, 128, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_encoder = DepthEncoder(resnet_pretrain=True, trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architescture\n",
    "### VisPosEmbeddings\n",
    "Input size: [batch_size, num_images, hidden_size]  \n",
    "embeddings = vis_embeddings + position_embeddings  \n",
    "Output size: [batch_size, num_images, hidden_size]  \n",
    "### CaptionBertAttention\n",
    "Attention layer in each block.  \n",
    "Input size: [batch_size, num_images, hidden_size]  \n",
    "Attention mask: [1, 1, num_images, num_images]  \n",
    "#### utils.get_attention_mask\n",
    "It receives number of neighbors allowed to attend to on each side specified in args.TRM_NEIGHBOR. And return a [1, 1, num_images, num_images] mask tensor, 1 indicates allowed attention, 0 indicates forbidden attention. \n",
    "\n",
    "In CaptionBertAttention, it first goes through CaptionBertSelfAttention, a self-attention layer which adds attention mask to the attention score matrix and outputs the context embeddings. Then it goes through a feedforward net which enables residue connection. \n",
    "\n",
    "Output size: [batch_size, num_images, hidden_size]\n",
    "\n",
    "### CaptionBertEncoder\n",
    "It consists of mutiple blocks of CaptionBertAttention specified in args.TRM_LAYER.\n",
    "\n",
    "### BertImgModel\n",
    "It simply calls CaptionBertEncoder.\n",
    "\n",
    "### WaypointBert\n",
    "It consists of a BertImgModel layer and a dropout layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinaryDistPredictor_TRM\n",
    "### Compression Layer\n",
    "* Receives input from RGBEncoder and DepthEncoder [batchsize * num_imgs, 2048, 7, 7], [batchsize * num_imgs, 128, 4, 4]\n",
    "* Flatten the input to 2048x7x7 and 128x4x4 respectively and compression to hiddem dim.\n",
    "* Reshape to (batchsize, num_imgs, hidden_dim)\n",
    "### Merge Layer\n",
    "* Concatenate the two outputs to (batchsize, num_imgs, 2*hidden_dim) and then linear project to (batchsize, num_imgs, hidden_dim) with ReLU.\n",
    "### Transformer Layer\n",
    "* Get attention mask specified in args.TRM_NEIGHBOR\n",
    "* Go through WaypointBert layer.\n",
    "### Classifier Layer\n",
    "* For each image in each sample in each batch, after Transformer Layer, its size is hidden_dim.\n",
    "* Linear layer from hidden_dim to hidden_dim with ReLU.\n",
    "* Linear layer hidden_dim to n_classes*(num_angles/num_imgs). For example, n_classes=12 is the distance index from current node to max radius. 0.25 to 3.0 is 12 * 0.25. num_angles=120 represents dividing 360 degrees to 120 * 3 degrees. num_imgs=12 is the number of images in the panorama. In this case, each image corresonds to num_angles/num_imgs=10 sectors of 3 degree. So this is a 10(3 degree) * 12(0.25m) heatmap centered at this image. In each sample, there are 12 images. So in total it's a 120(3 degree) * 12(0.25m) heatmap, where each image is responsible for its 10(3 degree) * 12(0.25m) local heatmap.\n",
    "* Output size is (batchsize, num_imgs, n_classes*(num_angles/num_imgs)). After reshape, it becomes (batchsize, num_angles, n_classes) which is the 120(3 degree) * 12(0.25m) heatmap. Each point in the heatmap correspond to a independent vis_logit which captures the probability of being a watpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = BinaryDistPredictor_TRM(args=args, hidden_dim=args.HIDDEN_DIM, n_classes=args.NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Dict\n",
    "* navigability_dict[scan_id][node] contains information about a node in this scene(scan_id)\n",
    "* target: A target map 120 * 12 . For the neighbors of this node, if its distance from the node <3.25m and >0.25m, its location in the target map is filled by 1. In the same angle, if there is more than one effective neighbor waypoints, retain the furthest waypoint so that it guarantees in each angle there is at most one waypoint. If there is no effective waypoints in each angle, delete this node. Then it uses gaussian filter to smooth the target map. It sets those locations in obstacles to be 0 according to the obstacle map. If all the values in target map are lower than some threshold, delete this node.\n",
    "* obstacle: A obstacle map 120 * 12. 1 indicates obstacles and 0 indicates open spaces.\n",
    "* weight: A weight map 120 * 12. Not used.\n",
    "* source_pos: The pose of the current node in the simulator.\n",
    "* target_pose: The poses of the effective neigbor waypoints in the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load navigability data (ground truth waypoints, obstacles, and weights) '''\n",
    "nav_dict_path = './training_data/%s_*_mp3d_waypoint_twm0.2_obstacle_first_withpos.json'%(args.ANGLES)\n",
    "navigability_dict = utils.load_gt_navigability(nav_dict_path)\n",
    "\n",
    "# Randomly select a scan_id and node to print\n",
    "print('navigability_dict.keys(): ', list(navigability_dict.keys()))\n",
    "scan_id = list(navigability_dict.keys())[0]\n",
    "print('navigability_dict[scan_id].keys(): ', list(navigability_dict[scan_id].keys()))\n",
    "node = list(navigability_dict[scan_id].keys())[0]\n",
    "print('navigability_dict[scan_id][node].keys(): ', navigability_dict[scan_id][node].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training images\n",
    "In ./training_data/rgbd_fov90/{split}/{scan}/{scan}_{node}_mp3d_imgs.pkl   \n",
    "It has 12 rgb and depth images for each node in each scan(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create data loaders for RGB and depth images '''\n",
    "train_img_dir = './training_data/rgbd_fov90/train/*/*.pkl'  # Training image directory\n",
    "traindataloader = RGBDepthPano(args, train_img_dir, navigability_dict)  # Training data loader\n",
    "eval_img_dir = './training_data/rgbd_fov90/val_unseen/*/*.pkl'  # Evaluation image directory\n",
    "evaldataloader = RGBDepthPano(args, eval_img_dir, navigability_dict)  # Evaluation data loader\n",
    "trainloader = torch.utils.data.DataLoader(traindataloader, \n",
    "        batch_size=args.BATCH_SIZE, shuffle=True, num_workers=4)  # Training batch data loader\n",
    "evalloader = torch.utils.data.DataLoader(evaldataloader, \n",
    "        batch_size=args.BATCH_SIZE, shuffle=False, num_workers=4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a single batch from the dataloader\n",
    "dataiter = iter(trainloader)\n",
    "batch = next(dataiter)\n",
    "\n",
    "# Select the first sample from the batch\n",
    "sample = {k: v[0] if isinstance(v, torch.Tensor) else v[0] for k, v in batch.items()}\n",
    "# Print sample information\n",
    "print(f\"Sample ID: {sample['sample_id']}\")\n",
    "print(f\"Scan ID: {sample['scan_id']}\")\n",
    "print(f\"Waypoint ID: {sample['waypoint_id']}\")\n",
    "print('Number of RGB images: ', len(sample['rgb']))\n",
    "print(f\"RGB shape: {sample['rgb'][0].shape}\")\n",
    "print(f\"Depth shape: {sample['depth'][0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_mse = torch.nn.MSELoss(reduction='none')\n",
    "params = list(predictor.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=args.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(trainloader):\n",
    "    data = batch\n",
    "    break\n",
    "\n",
    "scan_ids = data['scan_id']  # Scene IDs\n",
    "waypoint_ids = data['waypoint_id']  # Waypoint IDs\n",
    "rgb_imgs = data['rgb'].to(device)  # RGB images\n",
    "depth_imgs = data['depth'].to(device)  # Depth images\n",
    "\n",
    "print('input rgb shape: ', rgb_imgs.shape) #[B, N, C, H, W]\n",
    "print('input depth shape: ', depth_imgs.shape) #[B, N, H, W, 1]\n",
    "\n",
    "rgb_feats = rgb_encoder(rgb_imgs)      \n",
    "depth_feats = depth_encoder(depth_imgs)  \n",
    "\n",
    "print('output rgb_feats shape: ', rgb_feats.shape) #[B*N, 2048]\n",
    "print('output depth_feats shape: ', depth_feats.shape) #[B*N, 128, 4, 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, obstacle, weight, _, _ = utils.get_gt_nav_map(\n",
    "                    args.ANGLES, navigability_dict, scan_ids, waypoint_ids)\n",
    "target = target.to(device)\n",
    "obstacle = obstacle.to(device)\n",
    "weight = weight.to(device)\n",
    "print('target shape: ', target.shape) #(B, angles, 12)\n",
    "print('obstacle shape: ', obstacle.shape) #(B, angles, 12)\n",
    "print('weight shape: ', weight.shape) #(B, angles, 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MSE between predicted heat map and target map to define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_logits = TRM_predict('train', args, predictor, rgb_feats, depth_feats)\n",
    "loss_vis = criterion_mse(vis_logits, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newnew_dcvln38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
